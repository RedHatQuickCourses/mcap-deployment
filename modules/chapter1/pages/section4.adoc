= Ceph Storage Deployment

In this section, you will be deploying Ceph on a `storage` VM.
This `storage` VM will be a single node with three disks in the Ceph cluster.

image::MCAP_setup_3.png[]

== Prerequisites

`storage` VM is deployed and running.

== Ceph Storage Deployment Prerequisites

. Take the console of the `storage` VM and log in as _root_ user with _redhat_ as the password.
+
[source,bash,role=execute]
----
virsh console storage
----
+
.Sample output:
----
[root@hypervisor images]# virsh console storage
Connected to domain 'storage'
Escape character is ^] (Ctrl + ])

storage login: root
Password:
[root@storage ~]#
----

. Register the `storage` VM with a valid subscription.
You will need to provide your customer portal (access.redhat.com) credentials.
+
[source,bash,role=execute]
----
subscription-manager register
----
+
Disable all the repos.
+
[source,bash,role=execute]
----
subscription-manager repos --disable=*
----
+
Enable only required and Ceph repos.
+
[source,bash,role=execute]
----
subscription-manager repos --enable=rhel-9-for-x86_64-baseos-rpms --enable=rhel-9-for-x86_64-appstream-rpms --enable=rhceph-7-tools-for-rhel-9-x86_64-rpms
----
+
Check the repo list and ensure that all the required repos are enabled.
+
[source,bash,role=execute]
----
dnf repolist
----

. Update all packages on the `storage` VM to the latest version.
+
[source,bash,role=execute]
----
dnf -y update
----

. Install Ceph packages on the `storage` VM.
+
[source,bash,role=execute]
----
dnf -y install cephadm ceph-common lvm2 chrony podman
----

. Reboot the `storage` VM.
+
[source,bash,role=execute]
----
reboot
----

. Permit `root` login on `storage` VM.
This will allow the login to a `storage` VM as a `root` user using an `ssh` connection.
+
[source,bash,role=execute]
----
echo "PermitRootLogin yes" >> /etc/ssh/sshd_config
----
+
Restart the `sshd` service for configuration changes.
+
[source,bash,role=execute]
----
systemctl restart sshd
----

. Generate key using `ssh-keygen`, copy id to `storage` VM itself for password-less login.
This sets up SSH public key authentication to connect to a remote system.
+
[source,bash,role=execute]
----
ssh-keygen -t rsa -f /root/.ssh/id_rsa -N ''
----
+
[source,bash,role=execute]
----
ssh-copy-id storage.lab.example.com
----
+
[source,bash,role=execute]
----
eval $(ssh-agent)
----
+
`ssh-agent` is a background program that handles passwords for SSH private keys.
+
[source,bash,role=execute]
----
ssh-add ~/.ssh/id_rsa
----
+
The `ssh-add` command prompts the user for a private key password and adds it to the list maintained by `ssh-agent`.
Once you add a password to `ssh-agent`, you will not be prompted for it when using SSH or scp to connect to hosts with your public key.

. Create the `/etc/auth.json` file.
This file will be used in Ceph cluster deployment for getting access to the container image catalog.
+
[source,bash,role=execute]
----
cat >/etc/auth.json<<EOF
{
 "url":"registry.redhat.io",
 "username":"yourusername",
 "password":"yourpassword"
}
EOF
----
+
[NOTE]
Replace "yourusername" with your username and "yourpassword" with your password for registry.redhat.io.

== Ceph Configuration and Deployment

. Create the Ceph spec file `initial-config.yaml`, which is used as the initial configuration for Ceph cluster deployment.
There is only a single Ceph cluster node i.e. `storage` VM, used to deploy Ceph cluster.
You need to provide `storage` VM details in the spec file such as IP address, hostname, host, and three disks attached to the `storage` VM.
+
[source,bash,role=execute]
----
cat >initial-config.yaml<<EOF
---
service_type: host
addr: 192.168.122.9
hostname: storage.lab.example.com
---
service_type: mon
placement:
  hosts:
    - storage.lab.example.com
---
service_type: rgw
service_id: realm.zone
placement:
  hosts:
    - storage.lab.example.com
---
service_type: mgr
placement:
  hosts:
    - storage.lab.example.com
---
service_type: osd
service_id: default_drive_group
placement:
  host_pattern: 'storage*'
data_devices:
  paths:
    - /dev/vdb
    - /dev/vdc
    - /dev/vdd
EOF
----

. Deploy the Ceph storage cluster with the following command.
You will need to pass the spec file as `initial-config.yaml`, mon IP as `storage` VM's IP, and the registry json file as `/etc/auth.json`.
To deploy a Ceph cluster running on a single host, use the `--single-host-defaults` flag when bootstrapping.
+
[source,bash,role=execute]
----
cephadm bootstrap --apply-spec initial-config.yaml --mon-ip 192.168.122.9 --registry-json /etc/auth.json --allow-fqdn-hostname --single-host-defaults
----

. Verify deployed Ceph cluster.
+
[source,bash,role=execute]
----
/usr/sbin/cephadm shell
----
+
.Sample output:
----
Inferring fsid 47cd0eae-5e5c-11ef-a284-5254000aa988
Inferring config /var/lib/ceph/47cd0eae-5e5c-11ef-a284-5254000aa988/mon.storage/config
Using ceph image with id '3fd804e38f5b' and tag 'latest' created on 2024-07-31 19:44:24 +0000 UTC
registry.redhat.io/rhceph/rhceph-7-rhel9@sha256:75bd8969ab3f86f2203a1ceb187876f44e54c9ee3b917518c4d696cf6cd88ce3
[ceph: root@storage /]#
----
+
.Sample output:
----
[ceph: root@storage /]# ceph -s
  cluster:
    id:     47cd0eae-5e5c-11ef-a284-5254000aa988
    health: HEALTH_OK

  services:
    mon: 1 daemons, quorum storage (age 18m)
    mgr: storage.wudgfp(active, since 16m)
    osd: 3 osds: 3 up (since 16m), 3 in (since 17m)
    rgw: 1 daemon active (1 hosts, 1 zones)

  data:
    pools:   5 pools, 129 pgs
    objects: 191 objects, 453 KiB
    usage:   148 MiB used, 5.9 TiB / 5.9 TiB avail
    pgs:     129 active+clean
----
+
[NOTE]
You may have to wait for approximately 5 to 10 minutes for all the background processes needed for installation to complete and for the cluster to be in the `HEALTH_OK` state.
You may track the progress with watch `ceph -s` command.

. You may also run the `ceph health` command to verify cluster status.
+
.Sample output:
----
[ceph: root@storage /]# ceph health
HEALTH_OK
----

. In case of failure, you can use the following command to destroy the Ceph storage cluster.
+
[source,bash,role=execute]
----
cephadm rm-cluster --force --zap-osds --fsid `ceph fsid`
----