= MCAP Setup in RHDP Lab

== Overview

The MCAP solution is a combination of multiple Red Hat products working together to provide a robust, scalable, resilient, and highly available platform for an organizationâ€™s mission-critical applications.
The following Red Hat products are used in the MCAP solution:

Red Hat OpenShift Container Platform (OCP)

Red Hat OpenShift Virtualization (OCP Virt)

Red Hat Ansible Automation Platform (AAP)

Red Hat Advanced Cluster Management (RHACM)

Red Hat Advanced Cluster Security for Kubernetes (RHACS)

Red Hat OpenShift GitOps

== RHDP Lab Environment

[NOTE]
This setup is for Proof of Concepts (POC).
It is not recommended for Production use case.
For other architecture option, refer https://www.redhat.com/en/blog/nested-openshift-using-openshift-virtualization[Nested OpenShift using OpenShift Virtualization,window=read-later]

image::MCAP_setup.png[]

The following Red Hat products are used in this MCAP solution:

Red Hat OpenShift Container Platform (OCP)

Red Hat OpenShift Virtualization (OCP Virt)

Red Hat Advanced Cluster Management (RHACM)

=== Hardware and Bare-Metal

In this lab, Equinix bare metal is used.
The flavor is https://deploy.equinix.com/product/bare-metal/servers/[n3.xlarge.x86,window=read-later].

=== Kernel-based Virtual Machines (KVM)

In this MCAP solution, five Virtual Machines (VMs) are needed.

* One virtual machine as shared storage for _Tenant_ cluster.
* One virtual machine as _Hub_ cluster.
* Three virtual machines as _Infrastructure_ clusters.

=== Utility and Services

The bare metal is acting as hypervisor, http server, dhcp server and dns server.
All these services are configured on the bare metal.

=== Networking

As this setup uses KVMs as base infrastructure, all external communication and communication between clusters happen via virtual bridge on the bare metal.
nmstate operator is installed on the _Infrastructure_ clusters.
nmstate operator allows users to configure various network interface types, DNS and routing on cluster nodes.
The configuration is driven by two main object types, NodeNetworkConfigurationPolicy (Policy) and NodeNetworkConfigurationEnactment (Enactment).
In this lab setup, NodeNetworkConfigurationPolicy (Policy) and NetworkAttachmentDefinition (Object) are configured to connect the _Tenant_ cluster.

=== Storage

On the hypervisor, 7TB (3.5TB + 3.5TB disks) LV is created and mounted to storage pool directory path.
There is separate storage virtual machine (VM) is created as shared storage for _Tenant_ cluster.
This storage VM has three disks each of 2TB created from the 7TB LV.
These disks are used in Ceph deployment on storage VM.
OpenShift Data Foundation (ODF) operator is installed on the _Infrastructure_ clusters.
OpenShift Data Foundation Cluster for external Ceph storage system is created.

=== Hub Cluster

_Hub_ cluster is deployed as Single Node OpenShift (SNO) cluster.
_Hub_ cluster acts as central cluster which is installed with Multi cluster engine, Red Hat Advanced Cluster Management (RHACM) and LVMS operators.
Provisioning and Central Infrastructure Management (CIM) service are deployed on _Hub_ cluster.
_Infrastructure_ clusters and _Tenant_ cluster are deployed from _Hub_ cluster and added to _Hub_ cluster.

=== Infrastructure Cluster

There are three _Infrastructure_ clusters.
All are deployed as _Single Node OpenShift (SNO)_ cluster.
These clusters are deployed with OpenShift Virtualization, OpenShift Data Foundation (ODF) and nmstate operators.
Discovery ISOs are uploaded to all _Infrastructure_ clusters to deploy the _Tenant_ cluster nodes.

=== Nested OpenShift VMs using OpenShift Virtualization

Each _Infrastructure_ cluster has one virtual machine acts as OpenShift node in _Tenant_ cluster.

=== Tenant Cluster

There is only one _Tenant_ cluster.
This _Tenant_ cluster is deployed as _Three-Node OpenShift Compact_ cluster using virtual machines running on _Infrastructure_ clusters.
All applications and workloads are running on _Tenant_ cluster nodes.