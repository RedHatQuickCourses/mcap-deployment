= Install Sample Application on Tenant Cluster
:experimental:

In this section, you will be installing a basic _Node.js_ application on the _Tenant_ cluster.
You will also be testing the high availability and resilience of deployed _Node.js_ application.

image::MCAP_setup_1.png[]

== Prerequisites

. Ensure that the _OpenShift Data Foundation_ operator is installed.

. Ensure that the _Image Registry_ operator is using `Noobaa` storage with Red Hat OpenShift Data Foundation.
+
.Sample output:
----
[root@hypervisor ~]# oc get co image-registry
NAME             VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
image-registry   4.16.8    True        False         False      61s
----

== Install _Node.js_ application on _Tenant_ Cluster

. From the left navigation pane, click menu:Administrator[Developer].
+
image::tenant_console_switch_view.png[]

. You will be redirected to the developer view.
+
Click on the text btn:[create a Project].
+
image::tenant_console_developer_view.png[]

. In the create project window, provide the project details.
+
Name: `sample`.
+
Display name: `sample`.
+
Description: `This is a sample project`.
+
Click btn:[Create] to create the project.
+
image::tenant_console_create_project.png[]

. In the project view, click on the text btn:[Add page] to add the sample application.
+
image::tenant_console_nodejs_app_add.png[]

. In the add page window, click on the text btn:[View all samples].
+
image::tenant_console_nodejs_app_sample.png[]

. In the search section, search for the `basic node`.
+
Select the `Basic Node.js` application from the samples.
+
image::tenant_console_nodejs_app_search.png[]

. In the `Basic Node.js` application building window, keep all options as is and click btn:[Create] to create the application.
+
image::tenant_console_nodejs_app_create.png[]

. In the topology view, you will see application is created.
+
Click on the circle to view the application overview.
+
image::tenant_console_nodejs_app_create_1.png[]

. In resources tab, notice that the application is building.
+
If you click on the URL from the routes section, it will open the application page in a new tab.
+
image::tenant_console_nodejs_app_build_running.png[]

. Click btn:[Advanced...] to proceed.
+
image::tenant_console_nodejs_app_advanced_risk.png[]

. Click btn:[Accept the Risk and Continue] to accept the risk and proceed.
+
image::tenant_console_nodejs_app_accept_risk.png[]

. While the application is still building, note that the message displayed on the page is _Application is not available_.
+
image::tenant_console_nodejs_app_page_failure.png[]

. In a minute, you will notice the application is in the running state.
+
image::tenant_console_nodejs_app_build_success.png[]

. Refresh the application page to see the default page message.
+
image::tenant_console_nodejs_app_page_success.png[]

== Test the High Availability and Resilience

. Find out where the application pod is running.
+
.Sample output:
----
[root@hypervisor ~]# oc get pods -n sample -o wide
NAME                            READY   STATUS      RESTARTS   AGE     IP            NODE                   NOMINATED NODE   READINESS GATES
nodejs-basic-1-build            0/1     Completed   0          2m51s   10.130.0.63   tcn2.lab.example.com   <none>           <none>
nodejs-basic-6d55569c9c-gps2d   1/1     Running     0          2m51s   10.130.0.62   tcn2.lab.example.com   <none>           <none>
----

. In this case, the application pod is running on the `tcn2.lab.example.com` node.
+
This means that the `tcn2.lab.example.com` node is running on the `sno2` _Infrastructure_ cluster.

. To test high availability and resilience, first shut down the `sno2` VM, which will shutdown the _Infrastructure_ cluster.
+
image::tenant_console_nodejs_app_shutdwon_vm.png[]
+
Ensure that the `sno2` VM is in a shutoff state.
+
image::tenant_console_nodejs_app_shutdwon_vm_1.png[]

. If you refresh the application page, you will note the failure message again.
+
image::tenant_console_nodejs_app_page_failure.png[]

. The `pod-eviction-timeout` and `node-monitor-grace-period` parameters have the default values of `5m` and `40s` respectively.
This means it takes `5m40s` to trigger the pod eviction process after the last node status update.
+
After 5 minutes (eviction timeout), note that the application is successfully migrated to the `tcn1.lab.example.com` node.
+
.Sample output:
----
[root@hypervisor ~]# oc get pods -n sample -o wide
NAME                            READY   STATUS        RESTARTS   AGE   IP             NODE                   NOMINATED NODE   READINESS GATES
nodejs-basic-6d55569c9c-gps2d   1/1     Terminating   0          23m   10.130.0.62    tcn2.lab.example.com   <none>           <none>
nodejs-basic-6d55569c9c-tdkbm   1/1     Running       0          13m   10.128.1.115   tcn1.lab.example.com   <none>           <none>
----
+
[NOTE]
Eviction timeout - https://access.redhat.com/solutions/5359001[How to modify the pod reschedule timeout in OpenShift 4?,window=read-later]

. Refresh the application page again to view the default message again.
+
image::tenant_console_nodejs_app_page_success.png[]

. This test shows that even if one infrastructure node is down; the application automatically migrates to other infrastructure node.