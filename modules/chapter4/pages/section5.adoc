= Install Sample Application on Tenant Cluster
:experimental:

In this section, you will be installing basic _Node.js_ application on _Tenant_ cluster.
You will also be testing the high availability and resilience of deployed _Node.js_ application.

image::MCAP_setup_1.png[]

== Prerequisites

. Ensure _OpenShift Data Foundation_ operator is installed.

. Ensure _Image Registry_ operator is using `Noobaa` storage with Red Hat OpenShift Data Foundation.
+
.Sample output:
----
[root@hypervisor ~]# oc get co image-registry
NAME             VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
image-registry   4.16.8    True        False         False      61s
----

== Install _Node.js_ application on _Tenant_ cluster

. From the left navigation pane, click menu:Administrator[Developer].
+
image::tenant_console_switch_view.png[]

. You will be redirected to developer view.
+
Click on text btn:[create a Project].
+
image::tenant_console_developer_view.png[]

. In the create project window, provide the project details.
+
Name: `sample`.
+
Display name: `sample`.
+
Description: `This is sample project`.
+
Click btn:[Create] to create the project.
+
image::tenant_console_create_project.png[]

. In the project view, to add the sample application click on text btn:[Add page].
+
image::tenant_console_nodejs_app_add.png[]

. In the add page window, click on text btn:[View all samples].
+
image::tenant_console_nodejs_app_sample.png[]

. In the search section, search for `basic node`.
+
Select the `Basic Node.js` application from samples.
+
image::tenant_console_nodejs_app_search.png[]

. In `Basic Node.js` application building window, keep all options as is and click btn:[Create] to create the application.
+
image::tenant_console_nodejs_app_create.png[]

. In topology view, you will see application is created.
+
Click on the circle to view application overview.
+
image::tenant_console_nodejs_app_create_1.png[]

. In resources tab, notice the application is building.
+
If you click the URL from routes section, it will open the application page in new tab.
+
image::tenant_console_nodejs_app_build_running.png[]

. Click btn:[Advanced...] to proceed.
+
image::tenant_console_nodejs_app_advanced_risk.png[]

. Click btn:[Accept the Risk and Continue] to accept the risk and proceed.
+
image::tenant_console_nodejs_app_accept_risk.png[]

. As application is still building stage, you will notice the message as _Application is not available_ in page.
+
image::tenant_console_nodejs_app_page_failure.png[]

. In a minute, you will notice the application is in running state.
+
image::tenant_console_nodejs_app_build_success.png[]

. Refresh the application page to see the default page message.
+
image::tenant_console_nodejs_app_page_success.png[]

== Test the High Availability and Resilience

. Find out the where the application pod is running.
+
.Sample output:
----
[root@hypervisor ~]# oc get pods -n sample -o wide
NAME                            READY   STATUS      RESTARTS   AGE     IP            NODE                   NOMINATED NODE   READINESS GATES
nodejs-basic-1-build            0/1     Completed   0          2m51s   10.130.0.63   tcn2.lab.example.com   <none>           <none>
nodejs-basic-6d55569c9c-gps2d   1/1     Running     0          2m51s   10.130.0.62   tcn2.lab.example.com   <none>           <none>
----

. In this case, the application pod is running on the `tcn2.lab.example.com` node.
+
This means `tcn2.lab.example.com` node is running on `sno2` _Infrastructure_ cluster.

. To test high availability and resilience, shutdown the `sno2` _Infrastructure_ cluster and to do this you need to shutdown the `sno2` VM.
+
image::tenant_console_nodejs_app_shutdwon_vm.png[]
+
Ensure `sno2` VM is in shutoff state.
+
image::tenant_console_nodejs_app_shutdwon_vm_1.png[]

. If you refresh the application page, you will notice failure message again.
+
image::tenant_console_nodejs_app_page_failure.png[]

. The `pod-eviction-timeout` and `node-monitor-grace-period` parameters have the default value of `5m and 40s` respectively, which means by default, it takes `5m40s` to get pod eviction process triggered since the last status update from the node.
+
After five minutes i.e. eviction timeout, you will notice the application is successfully migrated to `tcn1.lab.example.com` node.
+
.Sample output:
----
[root@hypervisor ~]# oc get pods -n sample -o wide
NAME                            READY   STATUS        RESTARTS   AGE   IP             NODE                   NOMINATED NODE   READINESS GATES
nodejs-basic-6d55569c9c-gps2d   1/1     Terminating   0          23m   10.130.0.62    tcn2.lab.example.com   <none>           <none>
nodejs-basic-6d55569c9c-tdkbm   1/1     Running       0          13m   10.128.1.115   tcn1.lab.example.com   <none>           <none>
----
+
[NOTE]
Eviction timeout - https://access.redhat.com/solutions/5359001[How to modify the pod reschedule timeout in OpenShift 4?,window=read-later]

. Refresh the application page again to view the default message again.
+
image::tenant_console_nodejs_app_page_success.png[]

. This test shows, even though one infrastructure node is down; application automatically migrate to other infrastructure node.