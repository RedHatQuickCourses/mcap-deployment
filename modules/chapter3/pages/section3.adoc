= Access the Infrastructure Cluster
:experimental:

In this section, you will be accessing _Infrastructure_ clusters.

image::MCAP_setup_1.png[]

== Prerequisites

. Ensure _Infrastructure_ clusters (`sno1`, `sno2` and `sno3`) are deployed successfully.

. Create a directory as `/root/sno1/` on hypervisor.
+
[source,bash,role=execute]
----
mkdir /root/sno1
----

. Create `kubeadmin-password` file in `/root/sno1/` directory.
+
[source,bash,role=execute]
----
touch /root/sno1/kubeadmin-password
----

. Get the `kubeconfig` file and password for `kubeadmin` user from _Hub_ cluster console.
+
image::console_redhat_sno1_install_download.png[]

.. Download the `kubeconfig` file to hypervisor and then copy to `/root/sno1` directory on hypervisor.
+
.Sample output
----
[root@hypervisor ~]# mv Downloads/sno1-kubeconfig.yaml sno1/kubeconfig
----
+
.Sample output
----
[root@hypervisor ~]# ls -al sno1/
total 28
drwxr-xr-x.  2 root root  4096 Aug 22 15:20 .
dr-xr-x---. 13 root root  4096 Aug 22 15:18 ..
-rw-r--r--.  1 root root    24 Aug 22 15:11 kubeadmin-password
-rw-r--r--.  1 root root 12127 Aug 22 15:20 kubeconfig
----

.. Copy the password for `kubeadmin` user and paste it in new tab of firefox browser.
+
image::console_redhat_sno1_copy_password.png[]
+
Copy the password from tab of firefox browser and paste it in `/root/sno1/kubeadmin-password` file.
+
image::console_redhat_sno1_copy_password_1.png[]

[NOTE]
Follow same steps for `sno2` and `sno3` clusters.

== Access the _sno1_ Cluster via CLI

. Copy `/root/sno1/kubeconfig` file as `/root/.kube/config` file.
+
[source,bash,role=execute]
----
cp /root/sno1/kubeconfig /root/.kube/config
----

. Set `kubepass` variable as `kubeadmin` user's password.
+
[source,bash,role=execute]
----
kubepass=$(cat /root/sno1/kubeadmin-password)
----

. Login to _sno1_ cluster with `oc login` command.
+
[source,bash,role=execute]
----
oc login -u kubeadmin -p $kubepass
----
+
.Sample output
----
Login successful.

You have access to 79 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
----

. Review the nodes.
+
[source,bash,role=execute]
----
oc get nodes
----
+
.Sample output
----
NAME                  STATUS   ROLES                         AGE   VERSION
sno1.lab.example.com   Ready    control-plane,master,worker   10h   v1.29.7+6abe8a1
----

[NOTE]
Follow same steps for `sno2` and `sno3` clusters.

== Access the _sno1_ Cluster from Web Console

. Get the web console url from _Hub_ cluster console.
+
image::console_redhat_sno1_install_download.png[]
+
. Click on link from `Web Console URL`.
+
Click btn:[Advanced...] to proceed.
+
image::vnc_sno1_cluster_access_1.png[]
+
Click btn:[Accept the Risk and Continue] to proceed.
+
image::vnc_sno1_cluster_access_2.png[]
+
[NOTE]
You may need to accept the risk twice.

. Login as `kubadmin` user.
+
Get the `kubadmin` user's passwrod from from _Hub_ cluster console.
+
image::console_redhat_sno1_install_download.png[]
+
Copy the `kubadmin` user's passwrod from from _Hub_ cluster console and paste it in `Password` field.
+
image::vnc_sno1_cluster_access_3.png[]

. Once you logged in as `kubadmin` user, this is how the first screen looks like
+
image::vnc_sno1_cluster_access_4.png[]

. Verify _sno1_ cluster is in `Ready` state in _Hub_ cluster console.
+
image::vnc_sno1_cluster_access_5.png[]

[NOTE]
Follow same steps for `sno2` and `sno3` clusters.